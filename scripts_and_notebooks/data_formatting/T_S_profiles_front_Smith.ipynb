{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Jan 09 10:16 2023\n",
    "\n",
    "This is a script to cut out the T and S in the 50 km in front of the ice front for SMITH data\n",
    "\n",
    "@author: Clara Burgard\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-think",
   "metadata": {},
   "source": [
    "- calculate the distance to the ice front for the small domain in front of the ice shelf\n",
    "- take the ocean points at distance of ~50 km of the ice front "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-brain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm\n",
    "import gsw\n",
    "import matplotlib.pyplot as plt\n",
    "import multimelt.useful_functions as uf\n",
    "import multimelt.T_S_profile_functions as tspf\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "import distributed\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = distributed.Client(n_workers=12, dashboard_address=':8795', local_directory='/tmp', memory_limit='6GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-technical",
   "metadata": {},
   "source": [
    "READ IN THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf02249-4960-461f-9a78-c91b904c4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo_run = 'bi646' # 'bf663','bi646' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath_data='/bettik/burgardc/DATA/NN_PARAM/interim/SMITH_'+nemo_run+'/'\n",
    "inputpath_profiles='/bettik/burgardc/DATA/NN_PARAM/interim/T_S_PROF/SMITH_'+nemo_run+'/'\n",
    "inputpath_isf='/bettik/burgardc/DATA/NN_PARAM/interim/ANTARCTICA_IS_MASKS/SMITH_'+nemo_run+'/'\n",
    "\n",
    "# make the domain a little smaller to make the computation even more efficient - file isf has already been made smaller at its creation\n",
    "map_lim = [-3000000,3000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-minnesota",
   "metadata": {},
   "source": [
    "PREPARE MASK AROUND FRONT (TO RUN WITHOUT DASK!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_isf_points_from_line_small_domain(isf_points_da,line_points_da):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the distance between ice shelf points and a line.\n",
    "    \n",
    "    This function computes the distance between ice shelf points and a line. This line can be the grounding\n",
    "    line or the ice shelf front.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    whole_domain : xarray.DataArray\n",
    "        ice-shelf mask - all ice shelves are represented by a number, all other points (ocean, land) set to nan\n",
    "    isf_points_da : xarray.DataArray\n",
    "        array containing only points from one ice shelf\n",
    "    line_points_da : xarray.DataArray\n",
    "        mask representing the grounding line or ice shelf front mask corresponding to the ice shelf selected in ``isf_points_da``\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    xr_dist_to_line : xarray.DataArray\n",
    "        distance of the each ice shelf point to the given line of interest\n",
    "    \"\"\"\n",
    "    \n",
    "    # add a common dimension 'grid' along which to stack\n",
    "    stacked_isf_points = isf_points_da.stack(grid=['y', 'x'])\n",
    "    stacked_line = line_points_da.stack(grid=['y', 'x'])\n",
    "    \n",
    "    # remove nans\n",
    "    filtered_isf_points = stacked_isf_points[stacked_isf_points>0]\n",
    "    filtered_line = stacked_line[stacked_line>0]\n",
    "    \n",
    "    # write out the y,x pairs behind the dimension 'grid'\n",
    "    grid_isf_points = filtered_isf_points.indexes['grid'].to_frame().values.astype(float)\n",
    "    grid_line = filtered_line.indexes['grid'].to_frame().values.astype(float)\n",
    "    \n",
    "    # create tree to line and compute distance\n",
    "    tree_line = cKDTree(grid_line)\n",
    "    dist_yx_to_line, _ = tree_line.query(grid_isf_points)\n",
    "        \n",
    "    # add the coordinates of the previous variables\n",
    "    xr_dist_to_line = filtered_isf_points.copy(data=dist_yx_to_line)\n",
    "    # put 1D array back into the format of the grid and put away the 'grid' dimension\n",
    "    xr_dist_to_line = xr_dist_to_line.unstack('grid')\n",
    "    \n",
    "    return xr_dist_to_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93204984-a7a6-4721-a902-6428c27413cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_mask_orig = xr.open_dataset(inputpath_data+'mask_variables_of_interest_allyy_Ant_stereo.nc')#, chunks={'x': 600, 'y': 600})\n",
    "file_mask_orig = file_mask_orig.assign_coords({'time': range(1970, 1970+len(file_mask_orig.time))})#.chunk({'time': 1})\n",
    "file_mask = uf.cut_domain_stereo(file_mask_orig, map_lim, map_lim).isel(time=0).squeeze().drop('time')\n",
    "\n",
    "# only points below 1500 m\n",
    "offshore = file_mask['Bathymetry_isf'] > 1500 # .drop('lon').drop('lat')\n",
    "# only points above 1500 m\n",
    "contshelf = file_mask['Bathymetry_isf'] <= 1500 # .drop('lon').drop('lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "for timet in range(2001,1970 + 72):\n",
    "    print(timet)\n",
    "    T_S_ocean_file = xr.open_dataset(inputpath_profiles+'T_S_theta_ocean_corrected_'+str(timet)+'.nc').drop('time')\n",
    "    \n",
    "    file_isf_orig  = xr.open_dataset(inputpath_isf+'nemo_5km_isf_masks_and_info_and_distance_oneFRIS_'+str(timet)+'.nc')\n",
    "    nonnan_Nisf = file_isf_orig['Nisf'].where(np.isfinite(file_isf_orig['front_bot_depth_max']), drop=True).astype(int)\n",
    "    file_isf_nonnan = file_isf_orig.sel(Nisf=nonnan_Nisf)\n",
    "    large_isf = file_isf_nonnan['Nisf'].where(file_isf_nonnan['isf_area_here'] >= 2500, drop=True)\n",
    "    file_isf = file_isf_nonnan.sel(Nisf=large_isf).chunk(chunks={'x': 50, 'y': 50}).squeeze().drop('time')\n",
    "    if 'labels' in file_isf.coords.keys():\n",
    "        file_isf = file_isf.drop('labels')\n",
    "    \n",
    "    lon = file_isf['longitude']\n",
    "    lat = file_isf['latitude']\n",
    "    \n",
    "    ocean = np.isfinite(T_S_ocean_file['theta_ocean'].isel(depth=0)).drop('depth')\n",
    "     \n",
    "    # NB: 5.0 x 1.75 is the effective resolution at 70S for a model of 1 degree resolution in longitude (assuming 5 delta X and a Mercator grid)\n",
    "    mask_50km = (ocean & contshelf).load()\n",
    "    \n",
    "    lon_box = np.array([10.0])\n",
    "    lat_box = np.array([3.5])\n",
    "\n",
    "    close_region_around_isf_mask = tspf.mask_boxes_around_IF_new(lon, lat, mask_50km, \n",
    "                                    file_isf['front_min_lon'], file_isf['front_max_lon'], \n",
    "                                    file_isf['front_min_lat'], file_isf['front_max_lat'],  \n",
    "                                    lon_box, lat_box, \n",
    "                                    file_isf['isf_name'])\n",
    "    \n",
    "    dist_list = [ ]\n",
    "    for kisf in tqdm(file_isf['Nisf']):\n",
    "\n",
    "            if (file_isf['IF_mask']==kisf).sum() > 0:\n",
    "                region_to_cut_out = close_region_around_isf_mask.sel(Nisf=kisf).squeeze()\n",
    "                region_to_cut_out = region_to_cut_out.where(region_to_cut_out > 0, drop=True)\n",
    "                IF_region = file_isf['IF_mask'].where(file_isf['IF_mask']==kisf, drop=True)\n",
    "\n",
    "                dist_from_front = distance_isf_points_from_line_small_domain(region_to_cut_out,IF_region)\n",
    "                dist_list.append(dist_from_front)\n",
    "\n",
    "    dist_all = xr.concat(dist_list, dim='Nisf').reindex_like(file_isf)\n",
    "    dist_all.to_dataset(name='dist_from_front').to_netcdf(inputpath_profiles+'dist_to_ice_front_only_contshelf_'+str(timet)+'.nc')\n",
    "    \n",
    "    #del dist_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dedcc3-1722-4e99-8d31-51fdd78aceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for timet in range(1970,2051):\n",
    "    print(timet)\n",
    "    T_S_ocean_file = xr.open_dataset(inputpath_profiles+'T_S_theta_ocean_corrected_'+str(timet)+'.nc').drop('time')\n",
    "    \n",
    "    nisf_list = []\n",
    "    file_isf_orig  = xr.open_dataset(inputpath_isf+'nemo_5km_isf_masks_and_info_and_distance_oneFRIS_'+str(timet)+'.nc')\n",
    "    nonnan_Nisf = file_isf_orig['Nisf'].where(np.isfinite(file_isf_orig['front_bot_depth_max']), drop=True).astype(int)\n",
    "    file_isf_nonnan = file_isf_orig.sel(Nisf=nonnan_Nisf)\n",
    "    large_isf = file_isf_nonnan['Nisf'].where(file_isf_nonnan['isf_area_here'] >= 2500, drop=True)\n",
    "    file_isf = file_isf_nonnan.sel(Nisf=large_isf).chunk(chunks={'x': 50, 'y': 50}).squeeze().drop('time')\n",
    "    if 'labels' in file_isf.coords.keys():\n",
    "        file_isf = file_isf.drop('labels')\n",
    "    if 75 in large_isf:\n",
    "        nisf_list.append(75)\n",
    "    if 54 in file_isf.Nisf:\n",
    "        nisf_list.append(54)\n",
    "    file_isf = file_isf.sel(Nisf=nisf_list)\n",
    "    \n",
    "    lon = file_isf['longitude']\n",
    "    lat = file_isf['latitude']\n",
    "    \n",
    "    ocean = np.isfinite(T_S_ocean_file['theta_ocean'].isel(depth=0)).drop('depth')\n",
    "     \n",
    "    # NB: 5.0 x 1.75 is the effective resolution at 70S for a model of 1 degree resolution in longitude (assuming 5 delta X and a Mercator grid)\n",
    "    mask_50km = (ocean & contshelf).load()\n",
    "    \n",
    "    lon_box = np.array([10.0])\n",
    "    lat_box = np.array([3.5])\n",
    "\n",
    "    close_region_around_isf_mask = tspf.mask_boxes_around_IF_new(lon, lat, mask_50km, \n",
    "                                    file_isf['front_min_lon'], file_isf['front_max_lon'], \n",
    "                                    file_isf['front_min_lat'], file_isf['front_max_lat'],  \n",
    "                                    lon_box, lat_box, \n",
    "                                    file_isf['isf_name'])\n",
    "    \n",
    "    dist_list = [ ]\n",
    "    for kisf in file_isf['Nisf']:\n",
    "\n",
    "            if (file_isf['IF_mask']==kisf).sum() > 0:\n",
    "                region_to_cut_out = close_region_around_isf_mask.sel(Nisf=kisf).squeeze().load()\n",
    "                region_to_cut_out = region_to_cut_out.where(region_to_cut_out > 0, drop=True)\n",
    "                IF_region = file_isf['IF_mask'].where(file_isf['IF_mask']==kisf, drop=True)\n",
    "\n",
    "                dist_from_front = distance_isf_points_from_line_small_domain(region_to_cut_out,IF_region)\n",
    "                dist_list.append(dist_from_front)\n",
    "\n",
    "    dist_all = xr.concat(dist_list, dim='Nisf').reindex_like(file_isf)\n",
    "    dist_all.to_dataset(name='dist_from_front').to_netcdf(inputpath_profiles+'dist_to_ice_front_only_contshelf_'+str(timet)+'_only_AlexIsland.nc')\n",
    "    \n",
    "    #del dist_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32295e-ed6c-46db-814d-73981d9d851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for timet in tqdm(range(1970,2051)):\n",
    "    file_AI = xr.open_dataset(inputpath_profiles+'dist_to_ice_front_only_contshelf_'+str(timet)+'_only_AlexIsland.nc')\n",
    "    file_orig = xr.open_dataset(inputpath_profiles+'dist_to_ice_front_only_contshelf_'+str(timet)+'.nc')\n",
    "    \n",
    "    merged_file = xr.concat([file_orig.drop_sel(Nisf=75),file_AI], dim='Nisf')\n",
    "    \n",
    "    merged_file.to_netcdf(inputpath_profiles+'dist_to_ice_front_only_contshelf_'+str(timet)+'_merged75.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-flood",
   "metadata": {},
   "source": [
    "COMPUTING THE MEAN PROFILES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-stamp",
   "metadata": {},
   "source": [
    "CONTINENTAL SHELF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9ee1d-da01-46da-8179-9d9d60513e92",
   "metadata": {},
   "source": [
    "(needs ~ 18 x 6 GB of memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_domain_distkm = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1167238-cb03-488c-847e-47f60016ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for yy in tqdm(range(1970,1970 + 72)):\n",
    "    dist_to_front_file_yy = xr.open_dataset(inputpath_profiles+'dist_to_ice_front_only_contshelf_'+str(yy)+'_merged75.nc').chunk({'Nisf': 5})\n",
    "    T_S_ocean_file_yy = xr.open_dataset(inputpath_profiles+'T_S_theta_ocean_corrected_'+str(yy)+'.nc').chunk({'depth': 5})\n",
    "    \n",
    "    dist_to_front = dist_to_front_file_yy['dist_from_front']\n",
    "    mask_km = dist_to_front <= mask_domain_distkm\n",
    "    ds_sum = (T_S_ocean_file_yy * mask_km).sum(['x','y'])\n",
    "    \n",
    "    mask_depth = T_S_ocean_file_yy['salinity_ocean'].squeeze().drop('time') > 0\n",
    "    mask_all = mask_km & mask_depth\n",
    "    \n",
    "    mask_sum = mask_all.sum(['x','y'])\n",
    "    mask_sum = mask_sum.load()\n",
    "    \n",
    "    ds_mean = ds_sum/mask_sum\n",
    "    ds_mean.to_netcdf(inputpath_profiles+'T_S_mean_prof_corrected_km_contshelf_'+str(yy)+'.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
